---
title: "Weighted Cox Proportional Hazards Model"
author: "UNC EPID 722 Recitation, Ann Von Holle"
date: "February 29, 2016"
output:
  ioslides_presentation:
    css: shinyprezcss.css
    fig_height: 3
    fig_width: 3
    toc: yes
  beamer_presentation: default
csl: ../american-journal-of-epidemiology.csl
bibliography: ../bib1.bib
---

```{r setup, echo=FALSE}
# see http://stackoverflow.com/questions/24585254/working-with-knitr-using-subdirectories
  library(knitr)
  opts_knit$set(root.dir=normalizePath('../'))
  #opts_chunk$set(fig.path = "../figures/", dev='pdf') # corrected path and added dev
  opts_chunk$set(fig.width=12, fig.height=8, fig.align="left", echo=T, warning=FALSE, message=FALSE, comment=NA, results="hide", tidy=TRUE, tidy.opts=list(width.cutoff=60))
```

```{r, echo=FALSE}
### Specify packages for R

    #install.packages(c("knitr", "foreign", "tableone", "MCMCpack", "ipw")) # Note: you only need to do this once. then only if you want updates.
#  install.packages("data.table", "rms") # Watch out, rms loads a lot of other packages. skipping for now.
  library(data.table)
  library(reshape2)
  library(survival)
  library(ggplot2)
#  library(rms)
  library(muhaz)
  library(tableone)
  require(ipw)
  require(survminer) 
  set.seed(123) # set seed so you get same results each time you run.
```

```{r, echo=FALSE}
saspath <- 'C:/Program Files/SASHome/SASFoundation/9.4/sas.exe'
sasopts <- "-nosplash -log 'c:\\temp' -ls 80 -ps 60  -nocenter -nodate" # see http://bit.ly/1QB4ZTb
```

# Review of Cox PH model from EPID 722 2016 lecture and SAS code

NOTE: ALL SAS code below copied from 2016 EPID 722 lecture material. SAS code based on Steve Cole's program titled, "program7.17feb16.sas". R code below adapted to the SAS code by Ann Von Holle.

# Read file 

## Read file | SAS 
<!-- Note: the pipes only work for ioslides. -->
```{r s-read, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup', echo=TRUE}
libname mle "c:\temp";
%let dir = c:\temp;

* Read ASCII file;
data mle.b;
	infile "&dir\hividu15dec15.dat"; 
	input id 1-4 idu 6 white 8 age 10-11 cd4 13-16 drop 18 delta 20 @22 art 6.3 @29 t 6.3;
run;

* Export to .csv for use in R;
proc export data=mle.b outfile="c:\temp\hividu15dec15.csv" dbms=csv replace; run;

```

## Read file | R

Read the data (created in the SAS program above). 

```{r read, results='markup'}
#getwd() # get the working directory
b = read.csv("c:/temp/hividu15dec15.csv", header=T) # read in data.
```

# Look at data

## Look at data | SAS

```{r s-part1, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;

proc means data=a n mean sum min max; 
	var delta t drop idu white age cd4;
	title "Time from 12/6/95 to AIDS or death in WIHS";
run;
```

---

```{r s-part1, engine='sas', engine.path=saspath, engine.opts=sasopts, results="markup", echo=F}
```

## Look at data | R

```{r r-part1, results='markup'}
var.list = c("delta", "t", "drop", "idu", "white", "age", "cd4")

t.1 = CreateTableOne(data=b[,var.list]) # see https://cran.r-project.org/web/packages/tableone/vignettes/introduction.html
print(t.1, nonnormal=var.list)
```

# Background

Note: The following notation comes from lecture materials including "L3 Cox.pdf" lecture notes, program7.17feb16.sas, Cox model notes 15feb16.pdf. Also, Harrell book, chapter 20 [@harrell_regression_2015] and Kleinbaum book [@kleinbaum_survival_2012].

## Survival function

$$
S(t) = Prob\{T >t \} = 1 - F(t)
$$

$S(t)$ is the probability that an event will occur after time $t$ -- or that the individual will survive without the event at least until time t. Conversely, $F(t)$ is the risk that the event will occur before or at time $t$.

## Hazard function

$$
h(t) = \displaystyle\lim_{\Delta \to 0} \displaystyle \frac{Prob(t \leq T < t + \Delta \mid T \geq t)}{\Delta}
$$

"The hazard at time $t$ is related to the probability that the event will occur in a small interval around $t$, given that the event has not occured before time $t$." [@harrell_regression_2015, p403]

**Why rate?** because it's a probability per unit time, with scale of [0, \infty] [@kleinbaum_survival_2012].

If you know the hazard you will know the cumulative hazard, $H(t)$, and survival function, $S(t)$. Likewise, knowing the survival function enables you to calculate the hazard function.

$$
\int_0^t h(v)dv = H(t) = -log(S(t))
$$

## Multivariable Cox model

$$
\begin{align}
h_1(t \mid X) & = h_0(t) exp(\textbf{X} \mathbf{\beta}) \\
 & = h_0(t) exp(X_1 \beta_1 + X_2 \beta_2 + \ldots )
\end{align}
$$

where $h_0(t)$ is the baseline hazard function.

## Partial likelihood for \beta (without any ties or weights)

$$
L(\beta) = \displaystyle \prod_{Y_i \textrm{ uncensored}} \frac{exp(X_i \beta)}{\displaystyle\sum_{Y_j \geq Y_i} exp(X_j \beta)}
$$

**Note**: no intercept term in the partial likelihood. Cancels out. Cox model a model for relative hazard.

## Interpretation of hazard ratio


* Assume linearity and additivity of covariates on the log hazard (or cumulative hazard) scale.


* Assume that the relative hazard remains constant over time. NOT the hazard remaining constant over time. For example, the hazard function is constant in an exponential model, $h(t) = h$ with exponential distribution of time to event, $t$.

**Interpretation**: The index group has xx times the hazard of the referent group. The Cox PH model coefficients will provide the log(hazard ratio).


## Other areas of interest

There are other important topics to cover, but not in these slides. See "L3 Cox.pdf" document for start.

* Details on model fit diagnostics including residuals

* Model choice 

* Tied event times


# Crude Cox model

## Crude Cox model | SAS

```{r s-part2, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;

*Crude Cox model;
proc phreg data=a;
	model t*delta(0)=idu/rl ties=efron;
	*default is ties=breslow, ties=efron is better;
	ods select  parameterestimates;
*	ods select modelinfo fitstatistics parameterestimates;
	*this ods statement makes the print out nicer;
	title "Crude Cox model";
run; quit;

```

---

```{r s-part2, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup', echo=FALSE}
```

## Crude Cox model | R

```{r r-part2, results='markup'}
s.1 = coxph(Surv(t, delta) ~ idu, data=b, ties = "efron")
coef(summary(s.1))
```


## Adjusted Cox model | SAS

```{r s-part3, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;

*Adjusted Cox model;
proc phreg data=a;
	model t*delta(0)=idu age white cd4/rl ties=efron;
*	ods select modelinfo fitstatistics parameterestimates;
	ods select parameterestimates;
	title "Adjusted Cox model";
run; quit;
```

---

```{r s-part3, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup', echo=FALSE}
```

## Adjusted Cox model | R

```{r r-part3, results='markup'}
s.2 = update(s.1, formula = Surv(t, delta) ~ idu + age + white + cd4) # add confounder to crude model in s.1
coef(summary(s.2))
```

# Check proportional hazards assumption

See slide 36 of "L3 Cox.pdf" document for more details. 


## Check proportional hazards assumption, SAS, 1 {.codefont2}

```{r s-part3check-1, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;

*Assess PH assumption;
*First look at plot of log H(t) by t;
proc phreg data=a ;
	model t*delta(0)=;
	strata idu;
	baseline out=b cumhaz=caph/method=pl;
ods select parameterestimates;
data b; set b; if caph>0 then logcaph=log(caph);

ods listing gpath="&dir";
ods graphics / reset imagename="logcumhaz" imagefmt=jpeg height=5in width=5in;
proc sgplot data=b noautolegend;
	title "log H(t) by time";
	step x=t y=logcaph/group=idu;
run;
```

## Check proportional hazards assumption, SAS, 2

![](c:\temp\logcumhaz.jpeg) <!-- Add in the Survival.jpeg from SAS program above...-->

## Check proportional hazards assumption, SAS, 3 {.codefont2 .smaller}

```{r s-part3check-4, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;

*Second test product of idu and t;
proc phreg data=a;
	model t*delta(0)=idu idut / rl ties=efron;
	idut=idu*t; * should it be log time?;
  ods select parameterestimates;
	title "Crude Cox model, testing PH assumption";
run;
```

Second test looks at significance of interaction between time and **idu** effect. Book on survival analysis by Paul Allison [@allison_survival_2010] has good examples of handling interaction with time in SAS. (from page 153+)

## Check proportional hazards assumption, R, 1

NOTE: all code in this section for time dependent covariates copied from [this paper](https://www.jstatsoft.org/index.php/jss/article/view/v061c01/v61c01.pdf), titled "Tutorial: Survival Estimation for Cox Regression Models with Time-Varying Coefficients Using SAS and R"

```{r r-part3check-1, results='markup'}

cut.points <- unique(b$t[b$delta == 1]) # get unique time points for event

SURV2 <- survSplit(data = b, cut = cut.points, end = "t", start = "time0", event = "delta") # make counting process data
```

## Check proportional hazards assumption, R, 2

"To make the appearance match SAS, we sort SURV2 by subject then rename and reorder the columns." 

```{r r-part3check-2, results='markup'}
SURV2 <- SURV2[order(SURV2$id), ]
names(SURV2)
SURV2 <- SURV2[, c("id", "idu", "white", "age", "cd4", "time0", "t", "delta")]
names(SURV2)[7]="time1"
```

## Check proportional hazards assumption, R, 3

Add a time dependent covariate associated with **idu**.

```{r r-part3check-3, results='markup'}
SURV2$lt.idu = with(SURV2, idu*time1)
s.5 = coxph(Surv(time0, time1, delta) ~ idu + lt.idu, data=SURV2)
coef(summary(s.5))
```


## Check proportional hazards assumption, R, 2 (extra)

Also, you can look at [an R shiny app](https://vonholleunc.shinyapps.io/survival/) that allows you to modify parameters in a Cox model to see different types of proportional hazards assumptions violations.

[https://vonholleunc.shinyapps.io/survival/](https://vonholleunc.shinyapps.io/survival/)


## Nice table of Cox PH model assumptions [@harrell_regression_2015, p502] -- optional

<!-- see http://stackoverflow.com/questions/15625990/how-to-set-size-for-local-image-using-knitr-for-markdown-->

![](table20-8-harrell.png)


# Hazard and calculate crude Cox model by hand

## Calculate crude Cox model by hand (SAS) 
<!-- See http://stackoverflow.com/questions/30990262/applying-css-to-make-code-smaller-in-ioslides-style-->

See Appendix A, p2430, of assigned reading [@cole_survival_2010] for estimate of hazard, $h(t)$.

* $Y_k$ is the number of individuals who died at each of the ranked times $R_k$.

* $N_k$ is the number of individuals at-risk for mortality while under observation at distinct ranked event time $R_k$ for $k = 1, \ldots, D^{\prime}$.

$h_k = \frac{Y_k}{N_k \Delta_k}$ where $\Delta_k = R_k - R_{k-1}$ and $R_0=0$ and $R_k$ is observed event time at rank $k$.

The cumulative hazard, $H(t)$ is estimated as $H^{KM}_t = -log(S^{KM}(t))$.


## Calculate crude Cox model by hand (SAS), 1 {.codefont} 

Data set 'a' contains the survival data set.

Data set 'b' contains the survival function, $S(t)$ at each unique time point.

Data set 'c' provides the number at risk, $N_k$ for each unique time interval in the 'a' data set.

```{r s-part3a1, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;

*Get KM estimators of survival functions for each treatment group;
proc phreg data=a noprint;
	model t*delta(0)=;
	strata idu;
	baseline out=b survival=s / method=pl;
	output out=c(keep=idu t n) atrisk=n/method=pl;
*Merge together survival data with numbers at risk;
proc sort data=b;
	by idu descending s;
proc sort data=c nodups; by idu t;
data c; set c; by idu t; if first.t;
data mle.d; merge b c; by idu t; if s>.;
run;

title 'data set a';
proc print data=a(obs=10); run;

title 'data set b';
proc print data=b(obs=10); run;

title "data set c";
proc print data=c(obs=10); run;
```

## {.codefont}

```{r s-part3a1, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup', echo=FALSE}
```

## Calculate crude Cox model by hand (SAS), 2

Take data set 'd', merging b data set, with $S(t)$, and c data set, with $N_k$,  and find $h_k = \frac{Y_k}{N_k \Delta_k}$. The deltat variable = $\Delta_k$. Back-calculate $Y_k$ by taking -log($S(t)$) = $H(t)$. Then $H_{t_k} - H_{t_{k-1}} = Y_k$.

## Calculate crude Cox model by hand (SAS), 2 

```{r s-part3a2, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data d; set mle.d; run;

*Calculate the log hazard functions;
*This is a fairly complicated program, but largely repeats last session;
data mle.d(drop=chm1 sm1 tm1) avg(keep=avg0 avg1);
	set d end=end; 
	by idu descending s;
	retain chm1 0 sm1 1 tm1 0 sum0 0 sum1 0 count0 0 count1 0;
	if t=0 then do; 
		d=0; chm1=0; sm1=1; tm1=0; n=.;
	end;
	deltat=t-tm1; 
	if s>0 then ch=-log(s);
	y=round((ch-chm1)*n,1);
	if deltat>0 then h=y/(n*deltat);
	label n=;
	logh=log(h);
	if idu=0 and logh>. then do; count0=count0+y; sum0=sum0+logh; end;
	else if idu=1 and logh>. then do; count1=count1+y; sum1=sum1+logh; end;
	output mle.d;
	chm1=ch; sm1=s; tm1=t;
	if end then do; avg0=sum0/count0; avg1=sum1/count1; output avg; end;
run;

title 'data set d';
proc print data=mle.d(obs=10); run;
```

## Calculate crude Cox model by hand (SAS), 2 {.codefont}

```{r s-part3a2, engine='sas', engine.path=saspath, engine.opts=sasopts, results="markup", echo=FALSE}
```

## Calculate crude Cox model by hand (SAS), 3 

```{r s-part3a3, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data d; set mle.d; run;

ods listing gpath="&dir";
ods graphics/reset imagename="Hazards" imagefmt=jpeg height=6in width=8in;
proc sgplot data=d noautolegend;
	title "log hazard functions";
	loess x=t y=logh/smooth=.6 group=idu;
run;
```

---

![](c:\temp\Hazards.jpeg) <!-- Add in the Hazards.jpeg from SAS program above...-->


## Calculate crude Cox model by hand (R)  {.codefont2} 

```{r r-part3a, fig.keep='none'}

est.1 = survfit(Surv(t, delta) ~ 1 + strata(idu), data=b)
sum.surv = summary(est.1)

d = with(sum.surv, {
  data.frame( t=time,
              n=n.risk,
              r = 1-surv,
              s=surv,
              n.event = n.event,
              n.censor = n.censor,
              se = std.err,
              se2 = surv*sqrt((1-surv)/n.risk),
              idu = strata)
})
d$idu = ifelse(d$idu=="strata(idu)=idu=0", 0, 1)
head(d)
table(d$idu) # check

d.rev = rbind(
  c(t=0, n=d$n[1], r=NA, s=NA, n.event=0, n.censor=NA, se=NA, se2=NA, idu=0),
  c(t=0, n=d$n[1], r=NA, s=NA, n.event=0, n.censor=NA, se=NA, se2=NA, idu=1),
  d)# add a time origin point here for each strata.

# use data.table package to get time diff by strata. see http://bit.ly/1PV1wPn
d.rev.dt = data.table(d.rev)
setkey(d.rev.dt, idu)
d.rev.dt[, delta.t:= c(0,diff(as.matrix(t))), by=idu][,h:= n.event / (n*delta.t)]

some.vars = c("t", "n.event", "n", "delta.t", "h", "idu")
head(d.rev.dt[,some.vars, with=F]) # check with SAS numbers

p.haz = ggplot(data = d.rev.dt, aes(x=t, y=log(h), group=factor(idu), colour=factor(idu))) + 
  geom_point() +
  stat_smooth(method="loess", se=F) +
  scale_x_continuous(breaks = c(0,2,4,6,8,10)) +
  theme_bw() + 
  scale_colour_manual(values=c("blue", "red"), 
                      name="IDU", 
                      breaks=c(0,1), labels=c("non-user", "user")) +
  ggtitle("log hazard functions") +
  theme(text = element_text(size=20))

```

---

```{r r-part3a2, results='markup', echo=FALSE, fig.width=8, fig.height=6}
p.haz
```

# IP-weighted Cox model

## Confounding weights (SAS), 1 {.codefont .smaller}

Output the stabilized IP weight, $\displaystyle\frac{Pr(A_i=a)}{Pr(A_i=a \mid \mathbf{C})}$ where $A$ is the exposure (IDU in this example) and $\mathbf{C}$ = vector of covariates including **white**, **age**, **cd4** as confounders. The **n** data set contains the numerator of stabilized IP weight and the **d** data set contains the denominator of the stabilized IP weight.

```{r s-part4-1, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;

* Confounding weights;
proc logistic data=a desc noprint; model idu=; output out=mle.n p=num; run;
proc print data=mle.n(obs=10); run;

proc logistic data=a desc noprint; model idu=white age cd4; output out=mle.d p=den; run;
proc print data=mle.d(obs=10); run;
```

## Confounding weights (SAS), 2 {.codefont .small}

Merge those stabilized weights, **n** and **d**, with the source data set, **a**, to get the **c** data set.

```{r s-part4-2, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
%let dir = c:\temp;
data a; set mle.b; run;
data n; set mle.n; run;
data d; set mle.d; run;

data mle.c; merge a n d;
	if idu then w=num/den;
	else w = (1-num)/(1-den);
	label num= den=;
	drop _level_;
run; quit;

title "data set c";
proc print data=mle.c(obs=10); var id t num den w; run;
```


## Confounding weights (using the ipw package) (R), 1

```{r r-part4}
ipw.idu = ipwpoint(
  exposure = idu,
  family = "binomial",
  link = "logit",
  numerator = ~ 1,
  denominator = ~ white + age + cd4, # note if numerator unspecified the default is 1
  data=b)

cbind(b[1:10, c("id", "t")], ipw.idu$ipw.weights[1:10])
```


## Confounding weights (using the ipw package) (R), 2

```{r r-part4, results='markup', echo=FALSE}
```


## IP-censoring weights (SAS), 1  {.smaller .codefont2}

Take the data set with the IP weights, **w**, and get the quintiles for the censoring times: the data set named **q**.

```{r s-part5-1, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
%let dir = c:\temp;

*IP-censoring weights;
data mle.c; set mle.c; retain z 1; run;

proc univariate data=mle.c noprint; where drop=1; var t;
	output out=q pctlpts=20 40 60 80 pctlpre=p;
run;

data mle.q; set q; p0=0; p100=10; z=1;
proc print data=mle.q; run; * check quantiles to compare with R;
```

## IP-censoring weights (SAS), 2 {.codefont2 .small} 

Take those quintiles and make up to five separate rows per person in the **c** data set based on time intervals with cut points according to the quintiles. These data form the **e** data set. Each person has a max of five rows, but if they were censored or had an event before the last time point then that person only has however many rows in which they are still in the risk set.

```{r s-part5-2, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";
%let dir = c:\temp;
data q; set mle.q; run;
data c; set mle.c; run;

data mle.e; merge c q; by z;
	array j{6} p0 p20 p40 p60 p80 p100;
	do k=1 to 5;
		in=j(k);
		if j(k)<t<=j(k+1) then do; 
			out=t; 
			delta2=delta; *make a time-varying event indicator;
			_drop=drop; *make a time-varying drop indicator;
			output; 
		end;
		else if j(k+1)<t then do; out=j(k+1); delta2=0; _drop=0; output; end;
	end;
proc sort data=mle.e; by id in; run;
proc print data=mle.e(obs=10); run;
```

## IP-censoring weights (SAS), 2  {.codefont2 }

```{r s-part5-2, engine='sas', engine.path=saspath, engine.opts=sasopts, echo=F, results='markup'}
```

## IP-censoring weights (SAS), 3 {.codefont2 .small} 

Find the $Pr(\_drop=0 \mid in)$ where $in$ is equal to the five groups of time intervals and $\_drop$ is the drop out indicator variable, \_drop=1 if a person drops out at time $in$, 0 otherwise. This value equals the numerator of the drop out weight, **nm2**.

```{r s-part5-3, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
%let dir = c:\temp;
data e; set mle.e; run;

proc logistic data=e noprint; 
	class in/param=ref desc; 
	model _drop=in;
	output out=mle.nm2(keep=id _drop nm2 in out) prob=nm2;
run;
proc print data=mle.nm2(obs=10); run;
```

## IP-censoring weights (SAS), 4 {.codefont2}

Find the $Pr(\_drop=0 \mid in, idu, white, age, cd4)$. This value will be the denominator of the drop out weight, **dn2**.

```{r s-part5-4, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
%let dir = c:\temp;
data e; set mle.e; run;

proc logistic data=e noprint; 	
	class in/param=ref desc; 
	model _drop=in idu white age cd4;
	output out=mle.dn2(keep=id _drop dn2 in out) prob=dn2;
run;
proc print data=mle.dn2(obs=10); run;
```

## IP-censoring weights (SAS), 5 {.smaller} 

Merge the data sets with the numerator part of the censor weight, **dn2**, and the denominator part of the censor weight, **nm2**, by the unique person and time combinations. There are 1164 people and up to five time intervals per person so if there were no drop outs, no censoring and no events then there could be up to `r 1164*5` rows in the data set **f**.

For each person take the product of the weight across all the time intervals. If the person is censored in the time interval then their weight is 0.


$$
\textrm{ipcw}_{it}^{} = \displaystyle\frac{ Pr(D_i > t)}{Pr(D_i > t \mid \textrm{cov}_i^{})} = \displaystyle\prod_{q \leq t} \displaystyle\frac{Pr(D_i(q)=0)}{Pr(D_i(q)=0 \mid \mathbf{cov}_i)}
$$

  * Notation
  
    * $D_i(q)$ = drop out status for individual $i$ at time $q$ (1=yes, 0=no)
    
    * $Pr(D_i(q)=0)$ is "...marginal proportion who had not dropped out by time $q$." [@cole_estimation_2015]
    
    * $\mathbf{cov}_i$ = covariate values for individual $i$ that are common causes of drop out and study outcome for individual $i$ up to time $t$.
    

## IP-censoring weights (SAS), 5 {.codefont2}

```{r s-part5-5, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
%let dir = c:\temp;
data nm2; set mle.nm2; run;
data dn2; set mle.dn2; run;
data e; set mle.e(drop=num den); run; *if you do not take out num den variable associated with confounding weights they will mess up code that follows to make censoring weights;

proc sort data=nm2; by id in; 
proc sort data=dn2; by id in; 

data f; merge e nm2 dn2; by id in; retain num den;
	if first.id then do; num=1; den=1; end;
	num=num*nm2;
	den=den*dn2;
	if _drop then w2=(1-num)/(1-den);  else w2=num/den;
	w3=w*w2;
	label nm2= dn2=;
	run;

proc print data=f(obs=10); var id in out _drop nm2 dn2 num den w w2 w3; run;
data mle.f; set f; run; * make permanent data set for next step.;
```

## IP-censoring weights (SAS), 5 {.codefont2}

Summary of: 1) the IP weight, **w**, 2) the censoring weight, **w2**, and 3) the product of the two weights, **w3**.

```{r s-part5-6, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup'}
libname mle "c:\temp";
proc means data=mle.f; 
	var w w2 w3 num den;
	title "IP-weights";
	run;
```	


## IP-censoring weights (R) 

### Quantiles in R

```{r r-part5a, results='markup'}
# Set up the data frame with counting process style data
b$replicate=0
b.dt = data.table(b)
# try to get quantiles as close to SAS as possible
# documentation for quantile function in R says type 3
quantile(b.dt[b.dt$drop==1, with=T]$t, probs=c(0.20, 0.40, 0.60, 0.80), type=3) # check these quantiles with SAS. Close.
```

## IP-censoring weights (R), 1  {.codefont}

```{r r-part5b-1}
# function that can be used with replicates if you bootstrap. no boostrap in this example.
split.1 = function(i){
    quint = c(0, max(b$t), 
              as.numeric(quantile(b.dt[b.dt$replicate==i & 
                                         b.dt$drop==1, with=T]$t,
                                  probs=c(0.20, 0.40, 0.60, 0.80), type=3)))
    survSplit(data=b.dt[b.dt$replicate==i,], 
              cut=as.numeric(quint), 
              end="t", 
              start="time0", 
              event="delta") #survSplit function from the survival package will create a counting process style data frame for you.
  }

split.dat = split.1(0) # Replicate 0 is the observed data.

# IMPORTANT: Fix the drop variable. Only want the drop status (if =1) to show at last record
split.dat = split.dat[order(split.dat$id),] # order so I can check in SAS
first <- c(TRUE, diff(split.dat$id) !=0) #first id for each subject
last <- c(first[-1], TRUE) #last id 
split.dat$drop.rev = ifelse(last & split.dat$drop==1, 1, 0) 
#table(split.dat$drop.rev) # now matches 127 drops in SAS data

#split.dat[split.dat$replicate==0,][1:10,] # check results

# Use the reconfigured data to get censoring weights from ipw package.
head(split.dat)
ipw.cens.weights = ipwtm(
  exposure = drop.rev,
  family = "binomial",
  link = "logit",
  numerator = ~ factor(time0),
  denominator = ~ factor(time0) + idu  + white + age + cd4,
  id = id,
  tstart = time0,
  timevar = t,
  type = "first",
  data=split.dat)

#ipw.cens.weights$ipw.weights[1:10]

# Make a combo of the ipw and censor weights for weight in cox ph
x = data.frame(wt1 = ipw.idu$ipw.weights, id = b$id); #head(x)
y = data.frame(wt2 = ipw.cens.weights$ipw.weights, id = split.dat$id); #head(y)

wts = merge(x, y,
            by="id", all.x=T) # left outer join

head(wts)

wts$wt3 = with(wts, wt1*wt2)

```

## IP-censoring weights (R), 2

```{r r-part5b-2, results='markup'}
summary(wts[,c("wt1", "wt2", "wt3")])
```

## Check crude model is same with counting process data input (i.e. entry and exit times) (SAS), 1

```{r s-part6, engine='sas', engine.path=saspath, engine.opts=sasopts}
libname mle "c:\temp";

*Check crude model is same with counting-process data input (i.e. entry and exit times);

proc phreg data=mle.f;
	model (in,out)*delta2(0)=idu/rl ties=efron;
	*ods select modelinfo fitstatistics parameterestimates;
  ods select  parameterestimates;
	title "Check crude model";
run; quit;
```

## Check crude model is same with counting process data input (i.e. entry and exit times) (SAS), 2


```{r s-part6, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup', echo=FALSE}
```

## Check crude model is same with counting process data input (i.e. entry and exit times) (R)

```{r r-part6, results='markup'}
s.3 = coxph(Surv(time0, t, delta) ~ idu, data=split.dat, ties = "efron")
coef(summary(s.3))
coef(summary(s.1))  # original analysis with crude effect
```

## IP-weighted Cox model, with counting process data input. Use robust variance (SAS), 1 {.codefont2}

```{r s-part7, engine='sas', engine.path=saspath, engine.opts=sasopts}

libname mle "c:\temp";
data f; set mle.f; run;

proc means data=f;
 var in out delta2 idu w3;
run;

*IP-weighted Cox model, with counting-process data input;
*Use robust variance;

proc phreg data=f covs;
	id id;
	model (in,out)*delta2(0)=idu/rl ties=efron;
	weight w3;
	*ods select modelinfo fitstatistics parameterestimates;
  ods select parameterestimates;
	title "IP-weighted Cox model";
run; quit;

*proc print data=mle.f(obs=10);* run;

```

## IP-weighted Cox model, with counting process data input. Use robust variance (SAS), 2 {.codefont2 .smaller}

```{r s-part7, engine='sas', engine.path=saspath, engine.opts=sasopts, results='markup', echo=FALSE}
```

## IP-weighted Cox model, with counting process data input. Use robust variance (R)

```{r r-part7, results='markup'}
s.4 = coxph(Surv(time0, t, delta) ~ idu + cluster(id),
                data = split.dat, 
                weights = wts$wt3,
                ties="efron")
coef(summary(s.4))
```

## References {.smaller}

---
nocite: | 
  @buchanan_worth_2014
...
